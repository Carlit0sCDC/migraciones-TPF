 # Proyecto de An√°lisis de Flujos Migratorios y sus Impactos

<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/Logo.png" alt="Logo de Insight Analysts Collective">
</p>

## Descripci√≥n General

Somos **Insight Analysts collective**, un equipo de analistas de datos contratados por el Ministerio de Migraci√≥n de Canad√° para llevar a cabo este proyecto en el que se analizar√°n patrones de migraci√≥n en Am√©rica, revelando informaci√≥n importante sobre oportunidades en distintos aspectos a tener en cuenta (econ√≥mico, social, educativo, ambiental) para captar emigrantes y contribuir al enriquecimiento cultural y socioecon√≥mico del pa√≠s. A la vez se pondr√° a disposici√≥n los datos revelados por este an√°lisis para ayudar a los futuros emigrantes a tomar una decisi√≥n informada en cuanto a la migraci√≥n.

<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/mapa.png" alt="Logo de Insight Analysts Collective">
</p>

## √çndice

1. [Objetivo Principal](#objetivo-principal)
2. [Metodolog√≠a de trabajo](#metodolog√≠a-de-trabajo)
3. [üìú Sprint #1: Puesta en marcha del proyecto y Trabajo con Datos](#üìú-sprint-1-puesta-en-marcha-del-proyecto-y-trabajo-con-datos)
4. [ETL (Extracci√≥n Transformaci√≥n y Carga)](#etl-extracci√≥n-transformaci√≥n-y-carga)
5. [Exploraci√≥n de Datos (EDA)](#exploraci√≥n-de-datos-eda)
6. [KPIs (Indicadores Clave de Desempe√±o)](#kpis-indicadores-clave-de-desempe√±o)
7. [Stack tecnol√≥gico seleccionado](#stack-tecnol√≥gico-seleccionado)
8. [Presentaci√≥n sprint 1](#presentaci√≥n-sprint-1)
9. [üë®‚Äçüíª Sprint #2: Data Engineering](#üë®‚Äçüíª-sprint-2-data-engineering)
10. [Estructura de datos implementada (Data Lake)](#estructura-de-datos-implementada-data-lake)
11. [WorkFlow y tecnolog√≠as](#workflow-y-tecnolog√≠as)
12. [PIPELINE ETL (AWS CLOUD)](#pipeline-etl-aws-cloud)
13. [Validaci√≥n de datos](#validaci√≥n-de-datos)
14. [Diccionario de datos](#diccionario-de-datos)
15. [MVP Dashboard](#mvp-dashboard)
16. [Presentaci√≥n](#presentaci√≥n)

## Objetivo principal

El objetivo principal de este proyecto es desarrollar una estrategia integral que promueva la emigraci√≥n de la comunidad latina hacia Canad√°, en lugar de Estados Unidos, bas√°ndonos en datos objetivos y an√°lisis comparativos. Las meta son, en primer lugar, proporcionar al Ministerio de Migraciones de Canad√° insights relevantes para realizar cambios en su estructura que den como resultado el aumento de migrantes latinos hacia Canad√°. Y por otro lado, poner a disposici√≥n en el sitio de la embajada, √©ste mismo an√°lisis, que provea informaci√≥n precisa sobre las oportunidades y ventajas que ofrece Canad√° en t√©rminos de calidad de vida, empleo, educaci√≥n y pol√≠ticas de inmigraci√≥n, guiando a futuros emigrantes hacia un proceso de toma de decisiones informado."

## Metodolog√≠a de trabajo

En el marco del proyecto actual se ha adoptado la metodolog√≠a √°gil Scrum para la gesti√≥n y ejecuci√≥n de tareas. El proyecto consta de 3 sprints, cada uno est√° definido por un per√≠odo de tiempo de una semana durante la cual se desarrolla y entrega un conjunto de funcionalidades:

- üìú Sprint #1: Puesta en marcha del proyecto y Trabajo con Datos
- üë®‚Äçüíª Sprint #2: Data Engineering
- üìà Sprint #3: Data Analytics + ML

Esta metodolog√≠a nos permite usar un enfoque iterativo e incremental en el que d√≠a a d√≠a se ven los avances y agregados de valor al proyecto en conjunto con el Scrum Master. Luego al finalizar cada semana se culmina con el sprint mediante una presentaci√≥n del proyecto a nuestro Product Owner.

El **equipo de trabajo** est√° compuesto por un grupo de 4 desarrolladores:
* **Antonella Nieto** - Ingeniera de datos
* **Carlos D√≠as Colodrero** - Ingeniero de datos
* **Yuri D√≠az** - Analista de datos
* **Florencia Miranda** - Cient√≠fica de datos

Nuestro **cronograma de trabajo** fue gestionado a trav√©s de la plataforma Monday y lo pueden ver en en el siguiente [Enlace](https://flormiranda1995s-team.monday.com/boards/5064412581/).

<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/semana1.png" alt="Logo de Insight Analysts Collective">
</p>

# üìú Sprint #1: Puesta en marcha del proyecto y Trabajo con Datos

## ETL (Extracci√≥n Transformaci√≥n y Carga)
Para este trabajo utilizamos un dataset de la ONU, en espec√≠fico del Department of Economics and Social Affairs. El dataset nos proporciona datos cada 5 a√±os desde 1990 a 2020 con el destino de los migrantes, segmentado por regiones, continentes, nivel de ingresos y de desarrollo de los pa√≠ses receptores. En el siguiente
[link](https://www.un.org/development/desa/pd/content/international-migrant-stock) puede encontrarse m√°s informaci√≥n relacionada. Tambi√©n se puede encontrar el dataset titulado *undesa.csv* en la carpeta "data" del repositorio.

Los procesos realizados fueron los siguientes:

* El dataset constaba de 8 tablas. Las tablas de la 2 a la 8 representaban cada a√±o. Procedimos a crear una columna a√±o a cada tabla. 
* Unimos las tablas en un solo dataset.
* Sustitu√≠mos valores no num√©ricos y cambiamos los tipos de datos de las columnas.
* Cambiamos nombres de las columnas y eliminamos las no pertinentes para nuesto objetivo.
* Exportamos el dataset resultante limpio. 

## Exploraci√≥n de Datos (EDA)

Para comprender mejor la naturaleza de los flujos migratorios realizamos las siguientes indagaciones:

* Graficar la tendencia migratoria de la regi√≥n Latin America and the Caribbean 1990-2020.
* Extraer el aumento porcentual quinquenal y el promedio quinquenal hist√≥rico. 
* Observar la distribuci√≥n geogr√°fica de migrantes dentro la propia regi√≥n Latin America and the Caribbean.
* Segmentaci√≥n por nivel de ingreso de pa√≠ses de destino
* Top 2 de regiones a las que m√°s han migrado los latinoamericanos seg√∫n el nivel de ingresos y porcentaje hist√≥rico seg√∫n el nivel de ingresos. 
* Regiones preferidas por los migrantes latinoamericanos hist√≥ricamente. 

<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/aumento%20de%20la%20migracion%20de%20latinos%20en%20el%20tiempo.jpeg" alt="Variaci√≥n en la tasa de migraci√≥n de latinos a lo largo del tiempo" width="400"/> <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/variaci%C3%B3n%20de%20migracion%20de%20latinos%20al%20mundo.jpeg" alt="Migraci√≥n de latinos al mundo a lo largo del tiempo" width="400"/>
</p>

<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/distribuci%C3%B3n%20por%20nivel%20de%20ingreso%20paises%20destino.jpeg" alt="distribuci√≥n por nivel de ingresos del pais destino" width="400"/> <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/regiones%20preferidas%20para%20migrar.jpeg" alt="regiones preferidas por los latinos para migrar" width="400"/>
</p>


## KPIs (Indicadores Clave de Desempe√±o)

Hemos definido cuatro Indicadores Clave de Desempe√±o (KPIs) que ser√°n fundamentales para nuestro estudio:

1. **Vistas al informe**:
   
KPI: Conseguir 100 vistas del informe por mes

M√©trica: vistas al dashboard

C√°lculo del KPI: vistas_actuales/vistas_objetivo*100 (100)

2. **Aumento de migrantes Canad√°**:
   
KPI: Observar un aumento del 10% en la cantidad de migrantes desde pa√≠ses con alta tasa de alfabetizaci√≥n el pr√≥ximo a√±o

M√©trica: Cantidad de migrantes anual

C√°lculo del KPI: ((Migrantes a la fecha / Migrantes un a√±o atras) - 1) * 100

3. **Disminuci√≥n de migrantes EEUU** (Suponiendo que Canad√° captar√≠a estos migrantes)
   
KPI: Observar una disminuci√≥n del 3% en la cantidad de migrantes que eligen como destino EEUU provenientes de pa√≠ses con alta tasa de desempleo durante el pr√≥ximo a√±o

M√©trica: Cantidad de migrantes anual

C√°lculo del KPI: ((Migrantes a la fecha / Migrantes un a√±o atras) - 1) * 100

**Justificaci√≥n y Alusi√≥n a la Atracci√≥n para Inmigrantes en relaci√≥n con el PIB:**
Un aumento del PBI puede hacer que un pa√≠s sea m√°s atractivo para los inmigrantes, ya que suele estar relacionado con una mayor demanda de mano de obra y m√°s oportunidades econ√≥micas. El PIB es una medida clave de la salud econ√≥mica de un pa√≠s y, si el PBI de Canad√° aumenta significativamente en paralelo con un aumento en la tasa de migraci√≥n desde Am√©rica Latina, se podr√≠a argumentar que el pa√≠s ofrece un entorno econ√≥mico favorable y oportunidades de crecimiento, lo que a su vez atraer√≠a a m√°s migrantes en busca de una mejor calidad de vida y mayores perspectivas laborales. Este KPI busca establecer una conexi√≥n tangible entre el aumento del PIB y el inter√©s de los migrantes, demostrando c√≥mo un entorno econ√≥mico en crecimiento puede influir en la decisi√≥n de migrar hacia Canad√°.

4. **Aumento del PIB per c√°pita (Canad√°)**
   
KPI: Lograr un incremento del 5% en el PIB de Canad√° el pr√≥ximo a√±o

M√©trica: PIB per c√°pita anual

C√°lculo del KPI: ((PIB actual / PBI a√±o anterior) - 1) * 100

5. **Aumento del PIB en conjunto con el aumento en la tasa de migraci√≥n:**
   
KPI: Lograr un incremento del 5% en el PIB de Canad√° en el mismo per√≠odo en el que se observe un aumento del 10% en la tasa de migraci√≥n desde Am√©rica Latina del pr√≥ximo a√±o

M√©trica PBI: Cambio porcentual en el PBI de Canad√°

M√©trica Tasa de Migraci√≥n: Cambio porcentual en la tasa de migraci√≥n desde Am√©rica Latina hacia Canad√°

C√°lculo del KPI:

a- Calcular el cambio porcentual en el PBI de Canad√°: ((PBI actual - PBI anterior) / PBI anterior) * 100

b- Calcular el cambio porcentual en la tasa de migraci√≥n desde Am√©rica Latina: ((Tasa actual - Tasa anterior) / Tasa anterior) * 100

c- Evaluar si el aumento del PBI y el aumento en la tasa de migraci√≥n coinciden en el mismo per√≠odo y si la relaci√≥n es al menos 10:15 (considerando el aumento del 10% en PBI y el aumento del 15% en migraci√≥n)

d- Si se cumple la relaci√≥n, se considera que el KPI se ha logrado

## Stack tecnol√≥gico seleccionado:

- Data Pipeline y Data Lake: AWS Glue y S3 Amazon para la ingesta y transformaci√≥n de datos.
* Bibliotecas de Python: Pandas, Numpy, Seaborn, Matplotlib.
- Base de Datos: PostgreSQL para almacenar y gestionar datos.
- Data Warehouse: Amazon Redshift para an√°lisis de datos a gran escala.
- Dashboard: PowerBI para visualizaci√≥n interactiva de resultados.
- Modelos Predictivos: Python con bibliotecas como scikit-learn y TensorFlow.


# üë®‚Äçüíª Sprint #2: Data Engineering

## Estructura de datos implementada (Data Lake):

En nuestro proyecto de An√°lisis de Flujos Migratorios y sus Impactos, hemos optado por implementar un modelo no relacional, espec√≠ficamente un Data Lake, en lugar de un modelo relacional tradicional. Esta elecci√≥n se basa en una serie de fundamentos clave que respaldan nuestra decisi√≥n:

1. **Naturaleza de los Datos:**

Los datos que manejamos son heterog√©neos y no siguen una estructura fija de relaciones. Un modelo relacional tradicional ser√≠a inadecuado para representar eficazmente esta variedad de datos, que incluyen flujos migratorios y datos socioecon√≥micos de diversas fuentes.

2. **Escalabilidad y Flexibilidad:**

Un Data Lake, en particular Amazon S3, proporciona escalabilidad ilimitada y flexibilidad necesaria para manejar grandes vol√∫menes de datos no estructurados o semiestructurados. Esto es esencial ya que trabajamos con una amplia gama de datos que pueden crecer con el tiempo.

3. **Rendimiento de An√°lisis:**

La estructura del Data Lake permite la ejecuci√≥n de an√°lisis de datos avanzados y personalizados sin restricciones impuestas por un esquema de tabla predefinido. Esto es esencial para nuestro proyecto, donde necesitamos explorar datos complejos y buscar patrones espec√≠ficos.

## WorkFlow y tecnolog√≠as

El flujo de trabajo comienza con la carga de datos crudos en el Data Lake (bucket de S3 "Data inicial"). Luego, una primera funci√≥n Lambda realiza la transformaci√≥n inicial y carga los datos transformados en un segundo bucket de S3 llamado "Data Final". Una segunda funci√≥n Lambda se encarga de la carga incremental y la validaci√≥n de datos duplicados en el mismo "Data Final" bucket. Este flujo de trabajo garantiza que los datos se transformen, procesen y validen de manera eficiente antes de ser almacenados en el "Data Final" bucket, lo que proporciona un proceso de ETL escalable y confiable.

1) Creaci√≥n del Data-lake (Buckets data inicial y data final) - **AWS S3**
  
2) Creaci√≥n de funci√≥n para la carga inicial y transformaciones de datos crudos - **AWS Lambda**
  
3) Creaci√≥n de funci√≥n para la carga incremental de datos y validaci√≥n de duplicados - **AWS Lambda**
  
4) Conexi√≥n del bucket de data final con PowerBI - **AWS S3/Script de python**
  
5) MVP Dashboard con los datos extra√≠dos - **PowerBI**

## PIPELINE ETL (AWS CLOUD)

<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/PIPELINE.png" alt="PIPELINE">
</p>

## [Funci√≥n Lambda para ETL Automatizado:](https://github.com/Carlit0sCDC/migraciones-TPF/tree/main/lambda-etl)

La funci√≥n Lambda, denominada "lambda_handler," es una parte esencial de nuestro flujo de trabajo de procesamiento de datos en la nube. Esta funci√≥n se encarga de realizar la etapa de transformaci√≥n en el proceso ETL (Extracci√≥n, Transformaci√≥n y Carga) de datos. Perm√≠teme desglosar c√≥mo funciona en detalle:

1. **Detecci√≥n de Eventos:**

Lambda se encuentra en un estado de escucha activa, esperando eventos que desencadenen su ejecuci√≥n. En este caso, est√° configurada para responder a eventos relacionados con cambios en los "buckets" de Amazon S3. Cuando se carga un nuevo archivo en un "bucket" espec√≠fico, Lambda se inicia autom√°ticamente. Esta detecci√≥n de eventos es fundamental para el flujo de trabajo automatizado.

2. **Preparaci√≥n para el Trabajo:**

Antes de procesar los datos, Lambda necesita informaci√≥n sobre d√≥nde encontrar el archivo y d√≥nde colocar los resultados. Esto se logra a trav√©s de la configuraci√≥n previa, lo que permite a Lambda interactuar con los servicios de AWS.

3. **Recuperaci√≥n de Datos:**

Lambda accede al archivo en el "bucket" de S3 especificado y recupera los datos. Esto se realiza de manera eficiente y sin problemas, lo que garantiza que ning√∫n dato se pierda en el proceso.

4. **Transformaci√≥n de Datos:**

Aqu√≠ reside el n√∫cleo del trabajo de Lambda. Los datos brutos generalmente no est√°n en el formato ideal para su an√°lisis. Lambda realiza una serie de transformaciones seg√∫n las reglas especificadas. Por ejemplo, puede convertir cadenas en may√∫sculas, corregir datos err√≥neos y eliminar duplicados. Este proceso de transformaci√≥n es altamente personalizable y se adapta a las necesidades espec√≠ficas del proyecto.

5. **Almacenamiento de Datos Transformados:**

Una vez que los datos se han transformado con √©xito, Lambda los coloca en un nuevo lugar dentro del mismo o de otro "bucket" de S3. Los datos transformados est√°n ahora en un formato limpio y listos para su uso futuro.

6. **Finalizaci√≥n del Trabajo:**

Lambda completa su tarea y queda nuevamente en espera de eventos futuros. La velocidad y escalabilidad de Lambda permiten procesar grandes cantidades de datos en poco tiempo, lo que es esencial para un ETL √°gil y eficiente.

En resumen, esta funci√≥n Lambda realiza la fase de transformaci√≥n del ETL de manera eficaz, asegurando que los datos est√©n limpios y listos para el an√°lisis posterior. Su capacidad de respuesta a eventos y escalabilidad hacen que sea una herramienta poderosa en nuestro arsenal tecnol√≥gico para gestionar y transformar datos de manera automatizada.

## Validaci√≥n de datos

En nuestro proyecto, damos una gran importancia a la validaci√≥n de datos para garantizar la calidad y la integridad de la informaci√≥n que manejamos. Aqu√≠ hay algunas de las pr√°cticas de validaci√≥n de datos que implementamos:

- **Verificaci√≥n de Integridad**: Antes de la carga de datos en nuestro sistema, realizamos comprobaciones de integridad para asegurarnos de que los datos sean coherentes y cumplan con las reglas establecidas.

- **Limpieza de Datos**: Implementamos procesos de limpieza de datos para eliminar valores nulos, duplicados y datos inconsistentes. Esto nos permite trabajar con datos confiables.

- **Validaci√≥n de Formato**: Verificamos que los datos sigan el formato esperado, lo que incluye asegurarnos de que las fechas, n√∫meros, min√∫sculas y may√∫sculas y otros tipos de datos est√©n en el formato correcto.


- **Auditor√≠as y Registros**: Mantenemos registros de las validaciones realizadas, lo que nos permite rastrear cambios y problemas en los datos.


<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/validacion_1.jpeg" alt="validacion1">
</p>

<p align="center">
  <img src="https://github.com/Carlit0sCDC/migraciones-TPF/blob/main/img/validacion_2.jpeg" alt="validacion2">
</p>

adjuntar imagenes de tabla antes del etl y despues del etl aqui...


## Diccionario de datos

Este es el link del [Diccionario de datos.](https://docs.google.com/spreadsheets/d/1Kqbgxvg3pzMPJwpafznR0o5v2Fp2Tjm3/edit#gid=1757762391)
 Tambi√©n puede encontrarse  dentro de la carpeta "documentaci√≥n" del repositorio.
 

## MVP Dashboard

pegar screenshots dashboard aqui...


## Presentaci√≥n
A continuaci√≥n les dejamos el link a la [presentaci√≥n](https://www.canva.com/design/DAFs-WJPipA/HwFAwaGHidStTdXKRimmOQ/edit) del proyecto.


**Queremos enfatizar que este proyecto es completamente ficticio y no implica ninguna relaci√≥n de trabajo con el Ministerio de Migraciones ni con ninguna entidad gubernamental real. Tiene como √∫nico prop√≥sito fines educativos y de aprendizaje acad√©mico.**
